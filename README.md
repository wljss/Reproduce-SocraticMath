# Reproduce-SocraticMath

复现[论文](https://dl.acm.org/doi/10.1145/3627673.3679881)

要使用py文件的话把.ipynb文件一段段粘出来就行。

environment.yaml 是用到的python环境，使用conda导出的

导出python环境指令：

conda env export > environment.yaml

创建python环境指令：

conda env create -f environment.yaml

想要后台运行请使用以下命令：nohup python train.py &

后台运行输出会输出到nohup.out里

编写边学ing

### LoRA（Low-Rank Adaptation of Large Language Models）微调,大语言模型的低秩适应。

[论文链接](https://arxiv.org/abs/2106.09685)

主要用来让大模型适应特定的任务，传统全参数微调计算量大，且有过拟合风险。

主要思想：冻结原模型，不在原有的权重矩阵上直接更新，对于传统微调更新$W' = W + \Delta W$中的$\Delta W$，认为它是低秩的，可以用两个小得多的矩阵$B$和矩阵$A$的乘积$BA$表示，$B$的第二维(也就是$A$的第一维)大小就是LoRA秩r。冻结原模型，只训练$B$和$A$。

在微调期间，前向传播公式变为：$h = Wx + BAx$

参数，代码中LoraConfig部分:

rank（$r$）:LoRA秩，如上文所示，r越小，训练越快，能力可能会弱。论文里为64，本代码目前为8。

lora_alpha($\alpha$)：用于对$\Delta W$进行缩放,通常为LoRA秩的两倍。

Dropout：防止过拟合。

学习率和batch size：按照论文里分别设置为3e-4和64

附加LoRA适配器的模块：["q_proj", "v_proj", "gate_proj", "down_proj"]，分别是注意力机制的Query Projection和Value Projection，前馈神经网络FFN的Gate Projection和Down Projection。选择注意力层的 $Q$ 和 $V$ 矩阵，因为它们直接决定了模型如何看和理解输入。只微调 $Q$ 和 $V$ 通常就能取得很好的效果，并且参数量非常少。选择FFN是因为FFN被认为是模型存储知识的主要地方，微调它有助于模型更好地适应新任务。

结合deepseek自己记点笔记：

---

Dropout在训练阶段随机关闭一部分神经元，防止模型过拟合，提升泛华能力。每个神经元有$p$的概率被暂时关闭，保留概率为$1-p$。需要注意部分实现中保留的神经元输出会放大$\frac{1}{1-p}$倍，保证训练和测试时输出值期望一致。

---

查看模型有哪些模块:

```py
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("模型名字")

for name, module in model.named_modules():
    if isinstance(module, torch.nn.Linear):
        print(name)
```
---

基于卷积的编码是一种局部的编码方式，之间摸了输入数据的局部依赖关系。

循环神经网络RNN能够捕捉序列中的时间依赖关系，但是对长序列记忆能力弱，或出现梯度消失/梯度爆炸和信息传递容量的问题。

---

Qwen/Qwen1.5是decoder-only模型，基于Transformer架构设计.

原始的Transformer模型包含一个编码器（Encoder）和一个解码器（Decoder），主要用于序列到序列（Seq2Seq）的任务，如机器翻译。

- **Encoder**： 负责理解输入序列。它使用“双向自注意力”机制，可以同时看到某个词左右两边的所有上下文，从而获得对该词的“全面理解”。输出一个包含整个输入序列信息的上下文向量序列。

- **Decoder**： 负责生成输出序列。它使用“掩码自注意力”机制，只能看到当前词及其左边的所有词（即历史信息），而不能看到未来的词。这种设计使其非常适合用于自回归生成，即一个一个地生成下一个词。

Decoder-only模型顾名思义，只保留了原始Transformer中的解码器部分，并将其堆叠得非常深。它的设计哲学是：用一个统一的模型来完成理解和生成任务。你给它一个上文（提示），它就能生成下文（补全）。

一个标准的Decoder-only块主要由以下三个核心组件构成：

#### 1.掩码自注意力机制

这是与Encoder最根本的区别。它计算当前序列中每个位置与它**之前的所有位置**（包括自身）之间的关联度。在计算注意力分数后，会加上一个“注意力掩码矩阵”。这个矩阵通常是一个上三角矩阵，其主对角线及以下的值为0（或一个很小的负数，如-1e9），而上三角部分的值为负无穷（-inf）。这样，在经过Softmax层后，未来位置的权重就会趋近于0，从而被完全屏蔽。这样做的目的是确保模型在预测第 `i` 个词时，只能依赖于第 `1` 到第 `i-1` 个词。这模拟了文本生成时的真实场景，保证了模型的自回归特性。

需要注意，在原始Transformer-Decoder中，还有一个“编码器-解码器注意力”层（即交叉注意力），用于在生成每个目标词时关注编码器的输出。在Decoder-only模型中，这个交叉注意力层被移除了。这是因为没有编码器为其提供额外的信息源。所有的信息都来自于输入序列本身（即提示文本）和模型自身学到的知识。

#### 2.前馈神经网络

该网络位于自注意力层之后。通常是一个简单的两层全连接网络，中间包含一个非线性激活函数。第一个线性层将维度扩大，第二个线性层再投影回原来的模型维度。该模块的功能是对自注意力层输出的每个位置的表示进行独立、非线性的变换，增加模型的表达能力和复杂性。

#### 3.残差连接与层归一化

每个子层（自注意力和前馈网络）都被一个残差连接和层归一化所包围。残差连接是将子层的输入直接加到其输出上（`Output = LayerNorm(x + Sublayer(x))`）。这有助于缓解深度网络中的梯度消失问题，使模型能够训练得更深。层归一化对每个样本的每个层中的特征进行归一化，使训练过程更加稳定和快速。


训练的时候，输入是给定一个完整的文本序列，例如 `“我今天要去公园散步”`。文本被转换为词元（Token）序列和对应的位置编码。然后输入到Decoder-only模型中。模型通过掩码自注意力，从左到右依次处理每个词元。当处理到 `“散步”` 这个词时，它只能看到 `“我今天要去公园”` 这些信息。**目标**： 模型的任务是预测下一个词元。具体来说，输入序列被整体偏移一位，作为预测目标。模型需要努力让 `“我今天要去公园”` 的输出，尽可能准确地预测出下一个词是 `“散步”`。损失函数使用交叉熵损失函数，计算模型预测的概率分布与真实的下一个词（one-hot向量）之间的差异。

推理/生成阶段（自回归）的时候，输入给定一个提示（Prompt），例如 `“中国的首都是”`。迭代生成：模型接收当前的整个序列（初始就是提示文本），通过掩码自注意力计算，生成下一个词元的概率分布。从这个分布中采样一个词元，假设采样到的是 `“北京”`。将 `“北京”` 追加到输入序列的末尾，形成新的输入：`“中国的首都是北京”`。重复此过程，直到生成一个结束符或达到最大生成长度。这是一个典型的“自回归”过程，每次生成都依赖于之前生成的所有内容。

### Decoder-only架构优点

1.  **生成能力的统一**： 对于LLM来说，几乎所有任务（问答、翻译、摘要、对话）都可以被统一地建模为文本生成任务。给定一个输入（提示），生成一个输出。Decoder-only架构天生就是为生成而设计的。
2.  **强大的规模扩展性**： 研究表明，当模型参数量和数据量大幅增加时，Decoder-only架构展现出惊人的“涌现能力”，其性能提升是可预测的（ Scaling Laws）。这使得它非常适合构建超大规模模型。
3.  **训练效率**： 相比于Encoder-Decoder架构，纯解码器架构在预训练时计算更高效，因为它不需要维护两套参数，并且训练目标（下一个词预测）非常纯粹和简单。
4.  **零样本/少样本学习能力**： 通过巧妙的提示工程，Decoder-only模型能够在不更新参数的情况下，理解和执行新任务，这得益于其通过大量数据学到的强大语言建模能力。

### 5. 与其它架构的对比

| 架构类型 | 代表模型 | 核心特点 | 适用场景 |
| :--- | :--- | :--- | :--- |
| **Encoder-only** | BERT, RoBERTa | 双向上下文理解，能看到整个输入。 | 自然语言理解任务，如文本分类、情感分析、命名实体识别。通常不适合直接生成文本。 |
| **Encoder-Decoder** | T5, BART | 编码器理解输入，解码器基于编码器信息生成输出。 | 典型的序列到序列任务，如翻译、摘要、问答。需要将一种序列转换为另一种序列。 |
| **Decoder-only** | GPT, LLaMA | 单向上下文，自回归生成。 | **生成式任务**，如对话、创作、代码生成，以及通过提示完成的几乎所有NLP任务。 |

### 总结

Decoder-only模型架构是一种简洁、强大且高度可扩展的神经网络架构。它通过堆叠多层仅包含掩码自注意力和前馈网络的Transformer解码块，并采用**下一个词预测**作为训练目标。其核心优势在于将多样的NLP任务统一为自回归文本生成问题。