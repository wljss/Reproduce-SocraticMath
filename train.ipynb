{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db9c4a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前使用的是cuda/cpu?: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "\n",
    "#设置log文件\n",
    "logging.basicConfig(filename='log.txt', level=logging.INFO) #日志文件\n",
    "\n",
    "#设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"当前使用的是cuda/cpu?: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba3c96e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/fuxian/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前使用的是cuda/cpu?: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com' #防止无法下载的问题。linux请使用这个命令：export HF_ENDPOINT=https://hf-mirror.com\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "\n",
    "#设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"当前使用的是cuda/cpu?: {device}\")\n",
    "\n",
    "def load_model_and_tokenizer(): #加载模型和分词器\n",
    "    model_name = \"Qwen/Qwen1.5-7B\" #7B的显存要求要高一点\n",
    "    \n",
    "    #加载tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir='./myModels'\n",
    "    )\n",
    "    if tokenizer.pad_token is None: #pad_token是用来填充较短的序列的\n",
    "        tokenizer.pad_token = tokenizer.eos_token #结束token\n",
    "    \n",
    "    #加载模型\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        dtype=torch.float16,\n",
    "        device_map=\"auto\", #自动将模型分布到设备上\n",
    "        load_in_4bit=False,  #使用4位量化模型压缩技术，减少模型内存占用，稍微降低性能。用True会出问题，先不用\n",
    "        trust_remote_code=True, #允许从远程模型代码\n",
    "        cache_dir='./myModels', #下载的模型储存的位置\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "167bc494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "def setup_LoRA(model): #初始化LoRA，模型更新W' = W + ΔW中ΔW可以用两个小得多的矩阵B×A表示，B的第二维就是LoRA秩r。冻结原模型，只训练B和A\n",
    "    \n",
    "    model = prepare_model_for_kbit_training(model) #准备模型用于k-bit训练\n",
    "    \n",
    "    \n",
    "    lora_config = LoraConfig( #LoRA配置\n",
    "        r=8, #LoRA秩,很重要的一个参数。r越小训练越快，但拟合能力会下降\n",
    "        lora_alpha=16, #LoRA alpha参数，用于对ΔW进行缩放,通常为LoRA秩的两倍\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"gate_proj\", \"down_proj\"], #指定在哪些层应用LoRA，分别是注意力机制的Query Projection和Value Projection，前馈神经网络FFN的Gate Projection和Down Projection\n",
    "        lora_dropout=0.1, #正则化，dropout率，防止过拟合，\n",
    "        bias=\"none\", #不训练偏置参数\n",
    "        task_type=\"QUESTION_ANS\", #指定任务类型，这里是问答任务。CAUSAL_LM是对话，后面会试试\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config) #将LoRA适配器应用到原来的模型上\n",
    "\n",
    "    print('-----------------下面是可训练的参数-----------------')\n",
    "    model.print_trainable_parameters() #打印可训练的参数信息\n",
    "    print('-----------------上面是可训练的参数-----------------')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a7b66a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_math_teaching_dataset(): #加载数学教学数据集\n",
    "\n",
    "    \n",
    "    train_data = pd.read_csv(\"SocraticMath/data/csv/SocratesMATH.csv\", encoding='gbk', encoding_errors ='replace') #好像文件是GB2312编码，但会报错，用gbk也不行，所以encoding_errors ='replace'\n",
    "    teaching_dialogues= []\n",
    "    for data in train_data.values:\n",
    "        tmp = {\n",
    "            \"problem\": data[0],\n",
    "            \"dialogue\": data[1]\n",
    "        }\n",
    "        teaching_dialogues.append(tmp)\n",
    "    \n",
    "    return teaching_dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "443c133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "百度的Qwen1.5的对话模板如下:\n",
    "  <|im_start|>user\n",
    "  {message}<|im_end|>\n",
    "  <|im_start|>assistant\n",
    "  {message}<|im_end|>\n",
    "'''\n",
    "def format_data(example): #使用适合对话的格式，格式化数据。直接一股脑把对话数据全丢进去是不行的，要分开。\n",
    "    '''\n",
    "    错误写法：return f\"<|im_start|>user\\n{example['problem']}<|im_end|>\\n<|im_start|>assistant\\n{example['dialogue']}<|im_end|>\"\n",
    "    '''\n",
    "    dialogue_data=example['dialogue'].split('\\n')\n",
    "    formatted_dialogue=''\n",
    "\n",
    "    begin_flag=0\n",
    "    content=''\n",
    "\n",
    "    for line in dialogue_data:\n",
    "        line=line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        if line.startswith('学生：'):\n",
    "\n",
    "            if begin_flag==0:\n",
    "                begin_flag=1\n",
    "                content+=line[3:].strip()\n",
    "            else:\n",
    "                formatted_dialogue+=f'<|im_start|>assistant\\n{content}<|im_end|>\\n'\n",
    "                content=line[3:].strip()\n",
    "            \n",
    "            \n",
    "            #print('student_begin '+content)\n",
    "        \n",
    "        elif line.startswith('老师：'):\n",
    "\n",
    "            formatted_dialogue+=f'<|im_start|>user\\n{content}<|im_end|>\\n'\n",
    "            content=line[3:].strip()\n",
    "            #print('teacher_begin '+content)\n",
    "        else:\n",
    "            content+=line\n",
    "    \n",
    "    logging.info(formatted_dialogue)\n",
    "    return formatted_dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ab20814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def prepare_dataset(tokenizer): #准备训练数据\n",
    "\n",
    "    teaching_data = create_math_teaching_dataset() #加载数据\n",
    "    formatted_texts = [format_data(item) for item in teaching_data] #格式化数据\n",
    "    \n",
    "    def tokenize_function(examples): #对文本进行tokenization\n",
    "        tokenized = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True, #文本超过max_length时自动截断\n",
    "            padding=False,\n",
    "            max_length=1024, #增加长度以容纳对话\n",
    "            return_tensors=None, #返回Python列表\n",
    "        )\n",
    "        \n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy() #对于自回归语言模型，如GPT系列，训练目标是预测序列中的下一个token。因此：输入：[token1, token2, ..., token n-1]，标签：[token2, token3, ..., token n]，但实际上，在Hugging Face的Transformers库中，当labels设置为与input_ids相同时Trainer会自动处理偏移，计算损失时会忽略当前位置对自身的预测。\n",
    "        return tokenized\n",
    "    \n",
    "    dataset = Dataset.from_dict({\"text\": formatted_texts})#创建数据集\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function, #应用的函数，对每个批次进行tokenization\n",
    "        batched=True, #按批次处理数据，而不是逐条处理\n",
    "        batch_size=64, #论文里的batch_size\n",
    "        remove_columns=dataset.column_names, #移除原始列，只保留tokenize_function返回的列。最终数据集包含input_ids、attention_mask和labels\n",
    "    )\n",
    "    \n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95a7335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------正在加载模型和分词器-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------加载完成-----------------\n",
      "正在初始化LoRA\n",
      "-----------------下面是可训练的参数-----------------\n",
      "trainable params: 11,927,552 || all params: 7,733,252,096 || trainable%: 0.1542\n",
      "-----------------上面是可训练的参数-----------------\n",
      "-----------------初始化完成-----------------\n",
      "-----------------正在加载数据集-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6846/6846 [00:03<00:00, 2182.53 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 6846\n",
      "})\n",
      "-----------------加载完成-----------------\n",
      "-----------------正在设置训练参数-----------------\n",
      "-----------------设置完成-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22832/669144588.py:50: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/fuxian/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #加载模型和tokenizer\n",
    "    print(\"-----------------正在加载模型和分词器-----------------\")\n",
    "    model, tokenizer = load_model_and_tokenizer()\n",
    "    print(\"-----------------加载完成-----------------\")\n",
    "\n",
    "    #初始化LoRA\n",
    "    print(\"正在初始化LoRA\")\n",
    "    model = setup_LoRA(model)\n",
    "    print(\"-----------------初始化完成-----------------\")\n",
    "\n",
    "    #初始化数据集\n",
    "    print(\"-----------------正在加载数据集-----------------\")\n",
    "    train_dataset = prepare_dataset(tokenizer)\n",
    "    print(train_dataset)\n",
    "    print(\"-----------------加载完成-----------------\")\n",
    "\n",
    "    #设置训练参数\n",
    "    print(\"-----------------正在设置训练参数-----------------\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./qwen1.5-7b-math-teacher\", #输出目录\n",
    "        per_device_train_batch_size=1, #每个设备每次前向传播处理的批量大小\n",
    "        gradient_accumulation_steps=8, #累积多少次的梯度然后更新权重\n",
    "        num_train_epochs=5, #训练轮数\n",
    "        learning_rate=3e-4, #论文里的学习率\n",
    "        fp16=True, #启用混合精度训练，有的时候用16位浮点，有的时候32位，减少显存使用\n",
    "        logging_steps=10, #训练日志记录间隔\n",
    "        save_steps=200, #每训练200步保存一次检查点\n",
    "        eval_steps=200, #每200步在验证集上评估一次\n",
    "        save_total_limit=3, #最多只保留3个最新的检查点\n",
    "        remove_unused_columns=False, #保留数据集中所有列\n",
    "        run_name=\"qwen1.5-7b-math-teacher\",\n",
    "        report_to=None, #不向任何平台报告训练进度\n",
    "        warmup_steps=100, #预热步数，在训练开始时线性增加学习率，防止训练初期梯度爆炸\n",
    "        lr_scheduler_type=\"cosine\", #使用余弦退火学习率调度\n",
    "    )\n",
    "    print(\"-----------------设置完成-----------------\")\n",
    "    \n",
    "    #创建数据收集器\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False, #mlm=True用于BERT等模型的训练，随机掩盖部分token让模型预测，我们这里需要关闭\n",
    "    )\n",
    "    \n",
    "    #创建训练器\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    #开始训练\n",
    "    print(\"-----------------开始训练-----------------\")\n",
    "    trainer.train()\n",
    "    \n",
    "    #保存模型\n",
    "    print(\"-----------------训练结束，正在保存模型-----------------\")\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    \n",
    "    print(\"-----------------训练完成！-----------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fuxian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
